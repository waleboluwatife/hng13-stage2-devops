## Runbook – Observability & Alerts (Stage 3)

This runbook explains how to interpret and act on alerts generated by
the Stage 3 blue/green deployment.  Alerts are posted to Slack via
the webhook configured in the `.env` file.  Operators should follow
the guidance below when responding to notifications.

### Alert Types

#### 1. Failover Detected

**Slack message example:**

```
:rotating_light: Failover detected: blue → green at 2025‑10‑29T14:32:10Z
```

**Meaning:**  Traffic has switched from the primary pool to the
backup pool.  The primary may be unhealthy or experiencing errors.

**Operator actions:**

1. Identify which pool went down (the left side of the arrow).  In
   the example above, traffic moved from **blue** to **green**.
2. Check the health of the failed pool container.  Use `docker ps` to
   confirm it is running and inspect its logs:
   
   ```sh
   docker logs app_blue
   ```

3. Examine application logs or metrics for error messages.  If the
   issue is transient, allow the system to retry; Nginx will
   automatically fail back to the primary once it is healthy again.
4. If the primary remains unhealthy, investigate deployment issues,
   configuration changes, or bugs in the latest release.  Consider
   redeploying the previous stable image.
5. Once resolved, monitor for a recovery alert (optional) or verify
   manually that traffic has returned to the primary pool.

#### 2. High Error Rate

**Slack message example:**

```
:warning: High upstream error rate 12.50% over last 200 requests at 2025‑10‑29T14:40:05Z
```

**Meaning:**  More than the configured percentage of recent requests
have returned HTTP 5xx responses.  This indicates an elevated error
rate and potential instability in the service.

**Operator actions:**

1. Identify which pool is currently active by examining recent
   requests through the proxy:
   
   ```sh
   curl -I http://localhost:8080/version
   ```
   
   The `X‑App‑Pool` header will show `blue` or `green`.
2. View upstream container logs for error details:
   
   ```sh
   docker logs app_blue  # or app_green
   ```
3. Inspect application metrics and recent changes (deployments,
   configuration updates).  Roll back to a known good version if
   errors coincide with a new release.
4. Consider manually switching the active pool by editing
   `ACTIVE_POOL` in `.env` and restarting the Nginx container.  This
   should be done once the backup pool is confirmed healthy.
5. Continue to monitor the error rate.  A recovery alert may be sent
   once error levels drop below the threshold.

### Maintenance Mode

Setting `MAINTENANCE_MODE=true` in the `.env` file suppresses Slack
alerts.  Use this mode during planned maintenance, deployments or
tests when failovers or errors are expected.  Remember to disable
maintenance mode once operations return to normal so that alerts are
delivered again.

### Notes

- Alerts include UTC timestamps for correlation with logs and metrics.
- The watcher enforces a cooldown between alerts of the same type
  (`ALERT_COOLDOWN_SEC`) to prevent spamming during flapping events.
- Adjust `WINDOW_SIZE` and `ERROR_RATE_THRESHOLD` in `.env` to tune
  sensitivity.  Larger windows reduce false positives but may delay
  detection.
- The runbook is a living document; update it as new scenarios and
  procedures are discovered.